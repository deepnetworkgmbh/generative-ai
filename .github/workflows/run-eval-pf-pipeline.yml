name: Test and Evaulate Prompts with Promptflow

on:
  workflow_dispatch:
  push:
    branches: [ feature/cenkOpenAI ]

env: 
  GROUP: ${{secrets.GROUP}}
  WORKSPACE: ${{secrets.WORKSPACE}}
  SUBSCRIPTION: ${{secrets.SUBSCRIPTION}}
  RUN_NAME: web_classification_variant_1_20230816_215600_605116
  EVAL_RUN_NAME: classification_accuracy_eval_default_20230821_111809_077086
  COMPUTE_NAME: llm-devops-compute
  MANAGED_IDENTITY_NAME: test-promptflow-msi
  RUNTIME_NAME: promptflow-runtime
  LOCATION: westeurope

jobs:
  login-and-run-and-evalpf:
    runs-on: ubuntu-latest 
    steps:
    - name: Check out repo
      uses: actions/checkout@v2
    - name: Install az ml extension
      run: az extension add -n ml -y
    - name: Azure login
      uses: azure/login@v1
      with:
        creds: ${{secrets.AZURE_CREDENTIALS}}
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.11.4'
    - name: Install promptflow
      run: pip install -r flow/promptflow/web-classification/requirements.txt
    - name: Create Test Compute Source
      run: |
        az account set -s ${{env.SUBSCRIPTION}}
    - name: Run promptflow
      run: |
        pfazure run create -f flow/promptflow/web-classification/run.yml --subscription ${{env.SUBSCRIPTION}} -g ${{env.GROUP}} -w ${{env.WORKSPACE}} --stream
        pfazure run create -f flow/promptflow/web-classification/run.yml --subscription ${{env.SUBSCRIPTION}} -g ${{env.GROUP}} -w ${{env.WORKSPACE}} --stream > promptflow/llmops-helper/run_info.txt
        echo "RUN_NAME=$(python flow/promptflow/llmops-helper/parse_run_output.py run_info.txt)" >> "$GITHUB_ENV"
    - name: Show promptflow results
      run: pfazure run show-details --name ${{env.RUN_NAME}} --subscription ${{env.SUBSCRIPTION}} -g ${{env.GROUP}} -w ${{env.WORKSPACE}}
    - name: Run promptflow evaluations
      run: |
        pfazure run create -f flow/promptflow/web-classification/run_evaluation.yml --run ${{env.RUN_NAME}} --subscription ${{env.SUBSCRIPTION}} -g ${{env.GROUP}} -w ${{env.WORKSPACE}} --stream > flow/promptflow/llmops-helper/eval_info.txt
        echo "EVAL_RUN_NAME=$(python flow/promptflow/llmops-helper/parse_run_output.py eval_info.txt)" >> "$GITHUB_ENV"
    - name: Show promptflow evaluation Results
      run: |
        pfazure run show-metrics --name ${{env.EVAL_RUN_NAME}} --subscription ${{env.SUBSCRIPTION}} -g ${{env.GROUP}} -w ${{env.WORKSPACE}}
        pfazure run show-metrics --name ${{env.EVAL_RUN_NAME}} --subscription ${{env.SUBSCRIPTION}} -g ${{env.GROUP}} -w ${{env.WORKSPACE}} > flow/promptflow/llmops-helper/eval_result.json
    - name: Get assert eval results
      id: jobMetricAssert
      run: |
        export ASSERT=$(python flow/promptflow/llmops-helper/assert.py result.json 0.6)
        if ${ASSERT,,} ; then
          echo "::set-output name=result::true"
        else
          echo "::set-output name=result::false"
        fi
    - name: Register promptflow model
      if: ${{ steps.jobMetricAssert.outputs.result == 'true' }}
      run: az ml model create --file flow/promptflow/deployment/model.yaml  -g ${{env.GROUP}} -w ${{env.WORKSPACE}}
