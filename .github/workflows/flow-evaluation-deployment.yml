name: Test and Evaluate Prompts with Promptflow

on:
  workflow_dispatch:
  push:
    branches: [ feature/cenkOpenAI ]

env: 
  GROUP: ${{secrets.GROUP}}
  WORKSPACE: ${{secrets.WORKSPACE}}
  SUBSCRIPTION: ${{secrets.SUBSCRIPTION}}
  RUN_NAME: web_classification_variant_1_20230816_215600_605116
  EVAL_RUN_NAME: classification_accuracy_eval_default_20230821_111809_077086
  COMPUTE_NAME_PRE: llm-compute
  MANAGED_IDENTITY_NAME: test-promptflow-msi
  RUNTIME_NAME_PRE: llm-runtime
  LOCATION: germanywestcentral
  MODEL_NAME:  web-classification-model
  SP_USER: ${{secrets.SP_USER}}
  SP_PASS: ${{secrets.SP_PASS}}
  SP_TENANT: ${{secrets.SP_TENANT}}
  ENDPOINT_NAME: web-classification-endpoint
  DEPLOYMENT_NAME: deployment

jobs:
  evaluation:
    runs-on: ubuntu-latest
    outputs:
      deployModel: ${{ steps.EvalResult.outputs.eval_result }}
    steps:
    - name: Check out repo
      uses: actions/checkout@v2
    - name: Test Output
      id: EvalResult
      run: |
        echo "eval_result=true" >> $GITHUB_OUTPUT
        echo "::set-output name=eval_result::true"

  deployment:
    runs-on: ubuntu-latest
    needs:
      - evaluation
    if: needs.evaluation.outputs.deployModel == 'true'
    steps:
    - name: Test Output
      run: |
        echo ${{needs.evaluation.outputs.deployModel}}




